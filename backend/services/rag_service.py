import os
import uuid
import json
import pickle
from typing import List, Dict, Any, Optional
from datetime import datetime
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain.schema import Document

from config import Config

class RAGService:
    def __init__(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        # åˆ›å»ºå‘é‡å­˜å‚¨ç›®å½•
        os.makedirs(Config.VECTOR_STORE_PATH, exist_ok=True)
        
        # æŒä¹…åŒ–æ–‡ä»¶è·¯å¾„
        self.documents_file = os.path.join(Config.VECTOR_STORE_PATH, "documents.pkl")
        self.metadata_file = os.path.join(Config.VECTOR_STORE_PATH, "metadata.json")
        
        # åˆå§‹åŒ–å­˜å‚¨
        self.documents = {}  # {document_id: [Document]}
        self.document_metadata = {}
        
        # åŠ è½½å·²å­˜å‚¨çš„æ•°æ®
        self._load_documents()
        
        print(f"ğŸ“š RAGæœåŠ¡å·²å¯åŠ¨ï¼Œå·²åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£")
    
    def _save_documents(self):
        """ä¿å­˜æ–‡æ¡£åˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜æ–‡æ¡£å†…å®¹ï¼ˆä½¿ç”¨pickleä¿å­˜Documentå¯¹è±¡ï¼‰
            with open(self.documents_file, 'wb') as f:
                pickle.dump(self.documents, f)
            
            # ä¿å­˜å…ƒæ•°æ®ï¼ˆä½¿ç”¨JSONï¼‰
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.document_metadata, f, ensure_ascii=False, indent=2)
                
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(self.documents)} ä¸ªæ–‡æ¡£åˆ°ç£ç›˜")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡æ¡£å¤±è´¥: {str(e)}")
    
    def _load_documents(self):
        """ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        try:
            # åŠ è½½æ–‡æ¡£å†…å®¹
            if os.path.exists(self.documents_file):
                with open(self.documents_file, 'rb') as f:
                    self.documents = pickle.load(f)
                print(f"ğŸ“– ä»ç£ç›˜åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")
            
            # åŠ è½½å…ƒæ•°æ®
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.document_metadata = json.load(f)
                print(f"ğŸ“Š åŠ è½½äº† {len(self.document_metadata)} æ¡æ–‡æ¡£å…ƒæ•°æ®")
                
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ–‡æ¡£å¤±è´¥ï¼Œä½¿ç”¨ç©ºå­˜å‚¨: {str(e)}")
            self.documents = {}
            self.document_metadata = {}
    
    def add_documents(self, documents: List[Document], document_id: str) -> Dict[str, Any]:
        """æ·»åŠ æ–‡æ¡£åˆ°å­˜å‚¨"""
        try:
            if not documents:
                return {"success": False, "error": "æ²¡æœ‰æ–‡æ¡£å¯æ·»åŠ "}
            
            # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ å”¯ä¸€IDå’Œæ–‡æ¡£ID
            for i, doc in enumerate(documents):
                doc.metadata["doc_id"] = f"{document_id}_{i}"
                doc.metadata["document_id"] = document_id
                doc.metadata["created_at"] = datetime.now().isoformat()
            
            # æ·»åŠ åˆ°å†…å­˜å­˜å‚¨
            self.documents[document_id] = documents
            
            # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
            self.document_metadata[document_id] = {
                "document_id": document_id,
                "filename": documents[0].metadata.get("source", ""),
                "chunk_count": len(documents),
                "created_at": datetime.now().isoformat(),
                "total_chars": sum(len(doc.page_content) for doc in documents)
            }
            
            # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
            self._save_documents()
            
            return {
                "success": True,
                "message": f"æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£å—",
                "document_id": document_id,
                "chunk_count": len(documents)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}"
            }
    
    def search_similar_documents(self, query: str, k: int = 5, score_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        try:
            if not query.strip():
                return []
            
            query_lower = query.lower()
            results = []
            
            # éå†æ‰€æœ‰æ–‡æ¡£è¿›è¡Œç®€å•çš„æ–‡æœ¬åŒ¹é…
            for doc_id, docs in self.documents.items():
                for doc in docs:
                    content_lower = doc.page_content.lower()
                    
                    # ç®€å•çš„ç›¸å…³æ€§è®¡ç®—ï¼šæŸ¥è¯¢è¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°æ¬¡æ•°
                    query_words = query_lower.split()
                    matches = sum(1 for word in query_words if word in content_lower)
                    score = matches / len(query_words) if query_words else 0
                    
                    if score >= score_threshold:
                        results.append({
                            "content": doc.page_content,
                            "metadata": doc.metadata,
                            "score": score
                        })
            
            # æŒ‰ç›¸å…³æ€§æ’åºå¹¶è¿”å›å‰kä¸ªç»“æœ
            results.sort(key=lambda x: x["score"], reverse=True)
            return results[:k]
            
        except Exception as e:
            print(f"æœç´¢æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def get_documents_by_id(self, document_id: str) -> List[Dict[str, Any]]:
        """æ ¹æ®æ–‡æ¡£IDè·å–æ‰€æœ‰ç›¸å…³æ–‡æ¡£å—"""
        try:
            if document_id not in self.documents:
                return []
            
            documents = []
            for doc in self.documents[document_id]:
                documents.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata
                })
            
            return documents
            
        except Exception as e:
            print(f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def delete_document(self, document_id: str) -> Dict[str, Any]:
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        try:
            if document_id not in self.documents:
                return {"success": False, "error": "æ–‡æ¡£ä¸å­˜åœ¨"}
            
            chunk_count = len(self.documents[document_id])
            
            # åˆ é™¤æ–‡æ¡£
            del self.documents[document_id]
            
            # åˆ é™¤å…ƒæ•°æ®
            if document_id in self.document_metadata:
                del self.document_metadata[document_id]
            
            # ä¿å­˜æ›´æ”¹åˆ°æ–‡ä»¶
            self._save_documents()
            
            return {
                "success": True,
                "message": f"æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id}",
                "deleted_chunks": chunk_count
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}"
            }
    
    def clear_all_documents(self) -> Dict[str, Any]:
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
        try:
            total_chunks = sum(len(docs) for docs in self.documents.values())
            
            # æ¸…ç©ºæ–‡æ¡£
            self.documents.clear()
            
            # æ¸…ç©ºå…ƒæ•°æ®
            self.document_metadata.clear()
            
            # ä¿å­˜æ›´æ”¹åˆ°æ–‡ä»¶
            self._save_documents()
            
            return {
                "success": True,
                "message": "å·²æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£",
                "deleted_chunks": total_chunks
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"æ¸…ç©ºæ–‡æ¡£å¤±è´¥: {str(e)}"
            }
    
    def list_documents(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        try:
            return list(self.document_metadata.values())
            
        except Exception as e:
            print(f"åˆ—å‡ºæ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def get_vector_store_stats(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            total_chunks = sum(len(docs) for docs in self.documents.values())
            total_documents = len(self.documents)
            
            total_chars = sum(
                sum(len(doc.page_content) for doc in docs)
                for docs in self.documents.values()
            )
            
            return {
                "total_documents": total_documents,
                "total_chunks": total_chunks,
                "total_characters": total_chars,
                "average_chunk_size": total_chars // total_chunks if total_chunks > 0 else 0,
                "vector_store_path": Config.VECTOR_STORE_PATH
            }
            
        except Exception as e:
            return {
                "total_documents": 0,
                "total_chunks": 0,
                "total_characters": 0,
                "error": str(e)
            }
    
    def get_relevant_context(self, query: str, max_chars: int = 4000) -> str:
        """è·å–ä¸æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡"""
        try:
            # æœç´¢ç›¸å…³æ–‡æ¡£
            results = self.search_similar_documents(query, k=10)
            
            if not results:
                return ""
            
            # æŒ‰ç›¸å…³æ€§æ’åºå¹¶ç»„åˆä¸Šä¸‹æ–‡
            context_parts = []
            total_chars = 0
            
            for result in results:
                content = result["content"]
                if total_chars + len(content) > max_chars:
                    # å¦‚æœåŠ ä¸Šè¿™æ®µå†…å®¹ä¼šè¶…è¿‡é™åˆ¶ï¼Œæˆªå–éƒ¨åˆ†å†…å®¹
                    remaining_chars = max_chars - total_chars
                    if remaining_chars > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­—ç¬¦
                        content = content[:remaining_chars] + "..."
                    else:
                        break
                
                context_parts.append(content)
                total_chars += len(content)
                
                if total_chars >= max_chars:
                    break
            
            return "\n\n".join(context_parts)
            
        except Exception as e:
            print(f"è·å–ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {str(e)}")
            return "" 